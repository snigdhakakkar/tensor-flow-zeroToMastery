{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486c1053",
   "metadata": {},
   "source": [
    "## Milestone Project 1: üçîüëÅ Food Vision Big‚Ñ¢"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d1b3a",
   "metadata": {},
   "source": [
    "In the previous notebook (transfer learning part 3: scaling up) we built Food Vision mini: a transfer learning model which beat the original results of the Food101 paper with only 10% of the data.\n",
    "\n",
    "But you might be wondering, what would happen if we used all the data?\n",
    "\n",
    "Well, that's what we're going to find out in this notebook!\n",
    "\n",
    "We're going to be building Food Vision Big‚Ñ¢, using all of the data from the Food101 dataset.\n",
    "\n",
    "Yep. All 75,750 training images and 25,250 testing images.\n",
    "\n",
    "And guess what...\n",
    "\n",
    "This time **we've got the goal of beating [DeepFood](https://www.researchgate.net/publication/304163308_DeepFood_Deep_Learning-Based_Food_Image_Recognition_for_Computer-Aided_Dietary_Assessment)**, a 2016 paper which used a Convolutional Neural Network trained for 2-3 days to achieve 77.4% top-1 accuracy.\n",
    "\n",
    "üîë **Note: Top-1 accuracy** means \"accuracy for the top softmax activation value output by the model\" (because softmax ouputs a value for every class, but top-1 means only the highest one is evaluated). **Top-5 accuracy** means \"accuracy for the top 5 softmax activation values output by the model\", in other words, did the true label appear in the top 5 activation values? Top-5 accuracy scores are usually noticeably higher than top-1.\n",
    "![foodVisionBig.png](Images/foodVisionBig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8c9c9f",
   "metadata": {},
   "source": [
    "Table comparing difference between Food Vision Big (this notebook) versus Food Vision mini (previous notebook).\n",
    "\n",
    "Alongside attempting to beat the DeepFood paper, we're going to learn about two methods to significantly improve the speed of our model training:\n",
    "\n",
    "* Prefetching\n",
    "* Mixed precision training\n",
    "\n",
    "But more on these later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2308ef",
   "metadata": {},
   "source": [
    "### What we're going to cover\n",
    "\n",
    "* Using TensorFlow Datasets to download and explore data\n",
    "* Creating preprocessing function for our data\n",
    "* Batching & preparing datasets for modelling (**making our datasets run fast**)\n",
    "* Creating modelling callbacks\n",
    "* Setting up **mixed precision training**\n",
    "* Building a feature extraction model (see [transfer learning part 1: feature extraction](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/04_transfer_learning_in_tensorflow_part_1_feature_extraction.ipynb))\n",
    "* Fine-tuning the feature extraction model (see [transfer learning part 2: fine-tuning](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/05_transfer_learning_in_tensorflow_part_2_fine_tuning.ipynb))\n",
    "* Viewing training results on TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbff77c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
