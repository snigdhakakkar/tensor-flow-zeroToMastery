{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a17e8250",
   "metadata": {},
   "source": [
    "## Transfer Learning with TensorFlow Part 1: Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9138c18",
   "metadata": {},
   "source": [
    "We've built a bunch of convolutional neural networks from scratch and they all seem to be learning, however, there is still plenty of room for improvement.\n",
    "\n",
    "To improve our model(s), we could spend a while trying different configurations, adding more layers, changing the learning rate, adjusting the number of neurons per layer and more.\n",
    "\n",
    "However, doing this is very time consuming.\n",
    "\n",
    "Luckily, there's a technique we can use to save time.\n",
    "\n",
    "It's called **transfer learning**, in other words, taking the patterns (also called weights) another model has learned from another problem and using them for our own problem.\n",
    "\n",
    "There are two main benefits to using transfer learning:\n",
    "\n",
    "1. Can leverage an existing neural network architecture proven to work on problems similar to our own.\n",
    "2. Can leverage a working neural network architecture which has already **learned patterns** on similar data to our own. This often results in achieving great results with less custom data.\n",
    "What this means is, instead of hand-crafting our own neural network architectures or building them from scratch, we can utilise models which have worked for others.\n",
    "\n",
    "And instead of training our own models from scratch on our own datasets, we can take the patterns a model has learned from datasets such as [ImageNet](http://www.image-net.org/) (millions of images of different objects) and use them as the foundation of our own. Doing this often leads to getting great results with less data.\n",
    "\n",
    "Over the next few notebooks, we'll see the power of transfer learning in action.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a253a6b5",
   "metadata": {},
   "source": [
    "## What we're going to cover\n",
    "\n",
    "We're going to go through the following with TensorFlow:\n",
    "\n",
    "* Introduce transfer learning (a way to beat all of our old self-built models)\n",
    "* Using a smaller dataset to experiment faster (10% of training samples of 10 classes of food)\n",
    "* Build a transfer learning feature extraction model using TensorFlow Hub\n",
    "* Introduce the TensorBoard callback to track model training results\n",
    "* Compare model results using TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f96947",
   "metadata": {},
   "source": [
    "## Transfer leanring with TensorFlow Hub: Getting great results with 10% of the data\n",
    "\n",
    "If you've been thinking, \"surely someone else has spent the time crafting the right model for the job...\" then you're in luck.\n",
    "\n",
    "For many of the problems you'll want to use deep learning for, chances are, a working model already exists.\n",
    "\n",
    "And the good news is, you can access many of them on TensorFlow Hub.\n",
    "\n",
    "[TensorFlow Hub](https://tfhub.dev/) is a repository for existing model components. It makes it so you can import and use a fully trained model with as little as a URL.\n",
    "\n",
    "Now, I really want to demonstrate the power of transfer learning to you.\n",
    "\n",
    "To do so, what if I told you we could get much of the same results (or better) than our best model has gotten so far with only 10% of the original data, in other words, 10x less data.\n",
    "\n",
    "This seems counterintuitive right?\n",
    "\n",
    "Wouldn't you think more examples of what a picture of food looked like led to better results?\n",
    "\n",
    "And you'd be right if you thought so, generally, more data leads to better results.\n",
    "\n",
    "However, what if you didn't have more data? What if instead of 750 images per class, you had 75 images per class?\n",
    "\n",
    "Collecting 675 more images of a certain class could take a long time.\n",
    "\n",
    "So this is where another major benefit of transfer learning comes in.\n",
    "\n",
    "**Transfer learning often allows you to get great results with less data.**\n",
    "\n",
    "But don't just take my word for it. Let's download a subset of the data we've been using, namely 10% of the training data from the 10_food_classes dataset and use it to train a food image classifier on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7498fd39",
   "metadata": {},
   "source": [
    "![ImageNet_TransferLearning](Images/ImageNet_TransferLearning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdb95bc",
   "metadata": {},
   "source": [
    "## Downloading and becoming one with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5f73b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-12 23:52:23--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.33.176, 142.251.41.80, 172.217.165.16, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.33.176|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 168546183 (161M) [application/zip]\n",
      "Saving to: â€˜10_food_classes_10_percent.zipâ€™\n",
      "\n",
      "10_food_classes_10_ 100%[===================>] 160.74M  19.0MB/s    in 8.3s    \n",
      "\n",
      "2022-12-12 23:52:32 (19.4 MB/s) - â€˜10_food_classes_10_percent.zipâ€™ saved [168546183/168546183]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get data (10% of labels)\n",
    "import zipfile\n",
    "\n",
    "# Download data\n",
    "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
    "\n",
    "# Unzip the downloaded file\n",
    "zip_ref = zipfile.ZipFile(\"10_food_classes_10_percent.zip\", \"r\")\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac7a2b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 directories and 0 images in '10_food_classes_10_percent'.\n",
      "There are 10 directories and 0 images in '10_food_classes_10_percent/test'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'.\n",
      "There are 10 directories and 0 images in '10_food_classes_10_percent/train'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'.\n"
     ]
    }
   ],
   "source": [
    "# How many images in each folder?\n",
    "import os\n",
    "\n",
    "# Walk through 10 percent data directory and list number of files\n",
    "for dirpath, dirnames, filenames in os.walk(\"10_food_classes_10_percent\"):\n",
    "  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4c5769",
   "metadata": {},
   "source": [
    "Notice how each of the training directories now has 75 images rather than 750 images. This is key to demonstrating how well transfer learning can perform with less labelled images.\n",
    "\n",
    "The test directories still have the same amount of images. This means we'll be training on less data but evaluating our models on the same amount of test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b703eb02",
   "metadata": {},
   "source": [
    "## Creating data loaders (preparing the data)\n",
    "\n",
    "Now we've downloaded the data, let's use the ```ImageDataGenerator``` class along with the ```flow_from_directory``` method to load in our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b776ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Training images:\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n",
      "Found 750 images belonging to 10 classes.\n",
      "Testing images:\n",
      "Found 2500 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Setup data inputs\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "IMAGE_SHAPE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dir = \"10_food_classes_10_percent/train/\"\n",
    "test_dir = \"10_food_classes_10_percent/test/\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "test_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "print(\"Training images:\")\n",
    "train_data_10_percent = train_datagen.flow_from_directory(train_dir,\n",
    "                                               target_size=IMAGE_SHAPE,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               class_mode=\"categorical\")\n",
    "\n",
    "print(\"Testing images:\")\n",
    "test_data = train_datagen.flow_from_directory(test_dir,\n",
    "                                              target_size=IMAGE_SHAPE,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5f5045",
   "metadata": {},
   "source": [
    "## Setting up callbacks (things to run whilst our model trains)\n",
    "\n",
    "Before we build a model, there's an important concept we're going to get familiar with because it's going to play a key role in our future model building experiments.\n",
    "\n",
    "And that concept is **callbacks**.\n",
    "\n",
    "[Callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks) are extra functionality you can add to your models to be performed during or after training. Some of the most popular callbacks include:\n",
    "\n",
    "* **Experiment tracking with TensorBoard** - log the performance of multiple models and then view and compare these models in a visual way on TensorBoard (a dashboard for inspecting neural network parameters). Helpful to compare the results of different models on your data.\n",
    "* **Model checkpointing** - save your model as it trains so you can stop training if needed and come back to continue off where you left. Helpful if training takes a long time and can't be done in one sitting.\n",
    "* **Early stopping** - leave your model training for an arbitrary amount of time and have it stop training automatically when it ceases to improve. Helpful when you've got a large dataset and don't know how long training will take.\n",
    "We'll explore each of these overtime but for this notebook, we'll see how the TensorBoard callback can be used.\n",
    "\n",
    "The TensorBoard callback can be accessed using ```tf.keras.callbacks.TensorBoard()```.\n",
    "\n",
    "Its main functionality is ```saving a model's training performance metrics to a specified log_dir```.\n",
    "\n",
    "By default, logs are recorded every epoch using the ```update_freq='epoch'``` parameter. This is a good default since tracking model performance too often can slow down model training.\n",
    "\n",
    "To track our modelling experiments using TensorBoard, let's create a function which creates a TensorBoard callback for us.\n",
    "\n",
    "ðŸ”‘ Note: We create a function for creating a TensorBoard callback because as we'll see later on, each model needs its own TensorBoard callback instance (so the function will create a new one each time it's run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2d9e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensorboard callback (functionized because need to create a new one for each model)\n",
    "import datetime\n",
    "def create_tensorboard_callback(dir_name, experiment_name):\n",
    "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "      log_dir=log_dir\n",
    "  )\n",
    "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
    "  return tensorboard_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7001e8",
   "metadata": {},
   "source": [
    "Because you're likely to run multiple experiments, it's a good idea to be able to track them in some way.\n",
    "\n",
    "In our case, our function saves a model's performance logs to a directory named [dir_name]/[experiment_name]/[current_timestamp], where:\n",
    "\n",
    "* ```dir_name``` is the overall logs directory\n",
    "* ```experiment_name``` is the particular experiment\n",
    "*```current_timestamp``` is the time the experiment started based on Python's datetime.datetime().now()\n",
    "\n",
    "ðŸ”‘ Note: Depending on your use case, the above experimenting tracking naming method may work or you might require something more specific. The good news is, the TensorBoard callback makes it easy to track modelling logs as long as you specify where to track them. So you can get as creative as you like with how you name your experiments, just make sure you or your team can understand them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42f2371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
