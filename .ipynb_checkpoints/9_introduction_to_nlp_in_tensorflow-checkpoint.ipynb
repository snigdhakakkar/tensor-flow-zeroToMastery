{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1d95dc0",
   "metadata": {},
   "source": [
    "## Natural Language Processing with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d72c55",
   "metadata": {},
   "source": [
    "![nlp.png](Images/nlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74a9351",
   "metadata": {},
   "source": [
    "The main goal of [natural language processing (NLP)](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32) is to derive information from natural language.\n",
    "\n",
    "**Natural language** is a broad term but you can consider it to cover any of the following:\n",
    "\n",
    "1. **Text** (such as that contained in an email, blog post, book, Tweet)\n",
    "2. **Speech** (a conversation you have with a doctor, voice commands you give to a smart speaker)\n",
    "Under the umbrellas of text and speech there are many different things you might want to do.\n",
    "\n",
    "If you're building an email application, you might want to scan incoming emails to see if they're spam or not spam (classification).\n",
    "\n",
    "If you're trying to analyse customer feedback complaints, you might want to discover which section of your business they're for.\n",
    "\n",
    "🔑 Note: Both of these types of data are often referred to as sequences (a sentence is a sequence of words). So a common term you'll come across in NLP problems is called **seq2seq**, in other words, finding information in one sequence to produce another sequence (e.g. converting a speech command to a sequence of text-based steps).\n",
    "\n",
    "To get hands-on with NLP in TensorFlow, we're going to practice the steps we've used previously but this time with text data:\n",
    "\n",
    "Text -> turn into numbers -> build a model -> train the model to find patterns -> use patterns (make predictions)\n",
    "\n",
    " 📖 **Resource:** For a great overview of NLP and the different problems within it, read the article [A Simple Introduction to Natural Language Processing](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32).\n",
    "\n",
    "## What we're going to cover\n",
    "\n",
    "Let's get specific hey?\n",
    "\n",
    "* Downloading a text dataset\n",
    "* Visualizing text data\n",
    "* Converting text into numbers using tokenization\n",
    "* Turning our tokenized text into an embedding\n",
    "* Modelling a text dataset\n",
    "    * Starting with a baseline (TF-IDF)\n",
    "    * Building several deep learning text models\n",
    "        * Dense, LSTM, GRU, Conv1D, Transfer learning\n",
    "* Comparing the performance of each our models\n",
    "* Combining our models into an ensemble\n",
    "* Saving and loading a trained model\n",
    "* Find the most wrong predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea11a722",
   "metadata": {},
   "source": [
    "## Get helper functions\n",
    "\n",
    "In past modules, we've created a bunch of helper functions to do small tasks required for our notebooks.\n",
    "\n",
    "Rather than rewrite all of these, we can import a script and load them in from there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0231064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-15 02:11:28--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10246 (10K) [text/plain]\n",
      "Saving to: ‘helper_functions.py.2’\n",
      "\n",
      "helper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-12-15 02:11:28 (50.4 MB/s) - ‘helper_functions.py.2’ saved [10246/10246]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download helper functions script\n",
    "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1388f850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "# Import series of helper functions for the notebook\n",
    "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ae78d",
   "metadata": {},
   "source": [
    "### Examples of NLP:\n",
    "\n",
    "1. **One-to-many**: Image Captioning\n",
    "2. **Many-to-one**: Sentiment Analysis, Time Series Forecasting\n",
    "3. **Many-to-many**: Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb269ca",
   "metadata": {},
   "source": [
    "## Download a text dataset\n",
    "\n",
    "Let's start by download a text dataset. We'll be using the [Real or Not?](https://www.kaggle.com/c/nlp-getting-started/data) dataset from Kaggle which contains text-based Tweets about natural disasters.\n",
    "\n",
    "The Real Tweets are actually about disasters, for example:\n",
    "\n",
    "Jetstar and Virgin forced to cancel Bali flights again because of ash from Mount Raung volcano\n",
    "The Not Real Tweets are Tweets not about disasters (they can be on anything), for example:\n",
    "\n",
    "'Education is the most powerful weapon which you can use to change the world.' Nelson #Mandela #quote\n",
    "\n",
    "For convenience, the dataset has been [downloaded from Kaggle](https://www.kaggle.com/c/nlp-getting-started/data) (doing this requires a Kaggle account) and uploaded as a downloadable zip file.\n",
    "\n",
    "🔑 **Note**: The original downloaded data has not been altered to how you would download it from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34d47507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-15 02:18:34--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.32.80, 142.251.41.48, 172.217.1.16, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.32.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 607343 (593K) [application/zip]\n",
      "Saving to: ‘nlp_getting_started.zip’\n",
      "\n",
      "nlp_getting_started 100%[===================>] 593.11K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2022-12-15 02:18:34 (12.3 MB/s) - ‘nlp_getting_started.zip’ saved [607343/607343]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download data (same as from Kaggle)\n",
    "!wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\"\n",
    "\n",
    "# Unzip data\n",
    "unzip_data(\"nlp_getting_started.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cfaa95",
   "metadata": {},
   "source": [
    "Unzipping `nlp_getting_started.zip` gives the following 3 .csv files:\n",
    "\n",
    "* **sample_submission.csv** - an example of the file you'd submit to the Kaggle competition of your model's predictions.\n",
    "* **train.csv** - training samples of real and not real diaster Tweets.\n",
    "* **test.csv** - testing samples of real and not real diaster Tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4a1407",
   "metadata": {},
   "source": [
    "## Visualizing a text dataset\n",
    "\n",
    "Once you've acquired a new dataset to work with, what should you do first?\n",
    "\n",
    "Explore it? Inspect it? Verify it? Become one with it?\n",
    "\n",
    "All correct.\n",
    "\n",
    "Remember the motto: visualize, visualize, visualize.\n",
    "\n",
    "Right now, our text data samples are in the form of `.csv` files. For an easy way to make them visual, let's turn them into pandas DataFrame's.\n",
    "\n",
    "📖 **Reading**: You might come across text datasets in many different formats. Aside from CSV files (what we're working with), you'll probably encounter `.txt` files and `.json` files too. For working with these type of files, I'd recommend reading the two following articles by RealPython:\n",
    "\n",
    "[How to Read and Write Files in Python](https://realpython.com/read-write-files-python/)\n",
    "[Working with JSON Data in Python](https://realpython.com/python-json/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be8bb635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn .csv files into pandas DataFrame's\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3208a6",
   "metadata": {},
   "source": [
    "The training data we downloaded is probably shuffled already. But just to be sure, let's shuffle it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c6047f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle training dataframe\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42) # shuffle with random_state=42 for reproducibility\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85a543b",
   "metadata": {},
   "source": [
    "Notice how the training data has a \"target\" column.\n",
    "\n",
    "We're going to be writing code to find patterns (e.g. different combinations of words) in the \"text\" column of the training dataset to predict the value of the \"target\" column.\n",
    "\n",
    "The test dataset doesn't have a \"target\" column.\n",
    "\n",
    "Inputs (text column) -> Machine Learning Algorithm -> Outputs (target column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2cc2cb",
   "metadata": {},
   "source": [
    "![rnn.png](Images/ran.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2cc1026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The test data doesn't have a target (that's what we'd try to predict)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec9978",
   "metadata": {},
   "source": [
    "Let's check how many examples of each target we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c3e60cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many examples of each class?\n",
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5d8ae0",
   "metadata": {},
   "source": [
    "Since we have two target values, we're dealing with a binary classification problem.\n",
    "\n",
    "It's fairly balanced too, about `60%` negative class (target = 0) and `40%` positive class (target = 1).\n",
    "\n",
    "Where,\n",
    "\n",
    "`1` = a real disaster Tweet\n",
    "`0` = not a real disaster Tweet\n",
    "And what about the total number of samples we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a95366a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 7613\n",
      "Total test samples: 3263\n",
      "Total samples: 10876\n"
     ]
    }
   ],
   "source": [
    "# How many samples total?\n",
    "print(f\"Total training samples: {len(train_df)}\")\n",
    "print(f\"Total test samples: {len(test_df)}\")\n",
    "print(f\"Total samples: {len(train_df) + len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57e6b12",
   "metadata": {},
   "source": [
    "Alright, seems like we've got a decent amount of training and test data. If anything, we've got an abundance of testing examples, usually a split of 90/10 (90% training, 10% testing) or 80/20 is suffice.\n",
    "\n",
    "Okay, time to visualize, let's write some code to visualize random text samples.\n",
    "\n",
    "🤔 Question: Why visualize random samples? You could visualize samples in order but this could lead to only seeing a certain subset of data. Better to visualize a substantial quantity (100+) of random samples to get an idea of the different kinds of data you're working with. In machine learning, never underestimate the power of randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "861e9644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "Not being able to touch anything or anyone in Penneys without being electrocuted ??\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Watching Xela firefighters struggle to save burning buildings last night w/ old equipment makes me so grateful for DCFD @ChR3lyc @IAFF36\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "Injured Barcelona full-back Alba out of Super Cup http://t.co/jYiEgtnC6H\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "RNK issues Severe Thunderstorm Warning [wind: 60 MPH hail: 1.00 IN] for Rockingham Stokes [NC] till Aug 5 10:00 PM EDT  Û_\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "Womens Handbags Cross Body Geometric Pattern Satchel Totes Shoulder Bags White http://t.co/L1GFXgOZvx http://t.co/TKJYbjjsKl\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's visualize some random training examples\n",
    "import random\n",
    "random_index = random.randint(0, len(train_df)-5) # create random indexes not higher than the total number of samples\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n",
    "  _, text, target = row\n",
    "  print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
    "  print(f\"Text:\\n{text}\\n\")\n",
    "  print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3cb2cf",
   "metadata": {},
   "source": [
    "## Split data into training and validation sets\n",
    "\n",
    "Since the test set has no labels and we need a way to evalaute our trained models, we'll split off some of the training data and create a validation set.\n",
    "\n",
    "When our model trains (tries patterns in the Tweet samples), it'll only see data from the training set and we can see how it performs on unseen data using the validation set.\n",
    "\n",
    "We'll convert our splits from pandas Series datatypes to lists of strings (for the text) and lists of ints (for the labels) for ease of use later.\n",
    "\n",
    "To split our training dataset and create a validation dataset, we'll use Scikit-Learn's `train_test_split()` method and dedicate 10% of the training samples to the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "856d220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use train_test_split to split training data into training and validation sets\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
    "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
    "                                                                            test_size=0.1, # dedicate 10% of samples to validation set\n",
    "                                                                            random_state=42) # random state for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c744c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 6851, 762, 762)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the lengths\n",
    "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f393a302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first 10 training sentences and their labels\n",
    "train_sentences[:10], train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6993208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
