{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c973e2",
   "metadata": {},
   "source": [
    "## Natural Language Processing with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e8239a",
   "metadata": {},
   "source": [
    "![nlp.png](Images/nlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f961fa",
   "metadata": {},
   "source": [
    "The main goal of [natural language processing (NLP)](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32) is to derive information from natural language.\n",
    "\n",
    "**Natural language** is a broad term but you can consider it to cover any of the following:\n",
    "\n",
    "1. **Text** (such as that contained in an email, blog post, book, Tweet)\n",
    "2. **Speech** (a conversation you have with a doctor, voice commands you give to a smart speaker)\n",
    "Under the umbrellas of text and speech there are many different things you might want to do.\n",
    "\n",
    "If you're building an email application, you might want to scan incoming emails to see if they're spam or not spam (classification).\n",
    "\n",
    "If you're trying to analyse customer feedback complaints, you might want to discover which section of your business they're for.\n",
    "\n",
    "üîë Note: Both of these types of data are often referred to as sequences (a sentence is a sequence of words). So a common term you'll come across in NLP problems is called **seq2seq**, in other words, finding information in one sequence to produce another sequence (e.g. converting a speech command to a sequence of text-based steps).\n",
    "\n",
    "To get hands-on with NLP in TensorFlow, we're going to practice the steps we've used previously but this time with text data:\n",
    "\n",
    "Text -> turn into numbers -> build a model -> train the model to find patterns -> use patterns (make predictions)\n",
    "\n",
    " üìñ **Resource:** For a great overview of NLP and the different problems within it, read the article [A Simple Introduction to Natural Language Processing](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32).\n",
    "\n",
    "## What we're going to cover\n",
    "\n",
    "Let's get specific hey?\n",
    "\n",
    "* Downloading a text dataset\n",
    "* Visualizing text data\n",
    "* Converting text into numbers using tokenization\n",
    "* Turning our tokenized text into an embedding\n",
    "* Modelling a text dataset\n",
    "    * Starting with a baseline (TF-IDF)\n",
    "    * Building several deep learning text models\n",
    "        * Dense, LSTM, GRU, Conv1D, Transfer learning\n",
    "* Comparing the performance of each our models\n",
    "* Combining our models into an ensemble\n",
    "* Saving and loading a trained model\n",
    "* Find the most wrong predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f61780a",
   "metadata": {},
   "source": [
    "## Get helper functions\n",
    "\n",
    "In past modules, we've created a bunch of helper functions to do small tasks required for our notebooks.\n",
    "\n",
    "Rather than rewrite all of these, we can import a script and load them in from there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a874e4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-15 02:11:28--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10246 (10K) [text/plain]\n",
      "Saving to: ‚Äòhelper_functions.py.2‚Äô\n",
      "\n",
      "helper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-12-15 02:11:28 (50.4 MB/s) - ‚Äòhelper_functions.py.2‚Äô saved [10246/10246]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download helper functions script\n",
    "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e47ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "# Import series of helper functions for the notebook\n",
    "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38db0145",
   "metadata": {},
   "source": [
    "### Examples of NLP:\n",
    "\n",
    "1. **One-to-many**: Image Captioning\n",
    "2. **Many-to-one**: Sentiment Analysis, Time Series Forecasting\n",
    "3. **Many-to-many**: Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955c7d49",
   "metadata": {},
   "source": [
    "## Download a text dataset\n",
    "\n",
    "Let's start by download a text dataset. We'll be using the [Real or Not?](https://www.kaggle.com/c/nlp-getting-started/data) dataset from Kaggle which contains text-based Tweets about natural disasters.\n",
    "\n",
    "The Real Tweets are actually about disasters, for example:\n",
    "\n",
    "Jetstar and Virgin forced to cancel Bali flights again because of ash from Mount Raung volcano\n",
    "The Not Real Tweets are Tweets not about disasters (they can be on anything), for example:\n",
    "\n",
    "'Education is the most powerful weapon which you can use to change the world.' Nelson #Mandela #quote\n",
    "\n",
    "For convenience, the dataset has been [downloaded from Kaggle](https://www.kaggle.com/c/nlp-getting-started/data) (doing this requires a Kaggle account) and uploaded as a downloadable zip file.\n",
    "\n",
    "üîë **Note**: The original downloaded data has not been altered to how you would download it from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77c53edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-15 02:18:34--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.32.80, 142.251.41.48, 172.217.1.16, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.32.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 607343 (593K) [application/zip]\n",
      "Saving to: ‚Äònlp_getting_started.zip‚Äô\n",
      "\n",
      "nlp_getting_started 100%[===================>] 593.11K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2022-12-15 02:18:34 (12.3 MB/s) - ‚Äònlp_getting_started.zip‚Äô saved [607343/607343]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download data (same as from Kaggle)\n",
    "!wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\"\n",
    "\n",
    "# Unzip data\n",
    "unzip_data(\"nlp_getting_started.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8766bd36",
   "metadata": {},
   "source": [
    "Unzipping `nlp_getting_started.zip` gives the following 3 .csv files:\n",
    "\n",
    "* **sample_submission.csv** - an example of the file you'd submit to the Kaggle competition of your model's predictions.\n",
    "* **train.csv** - training samples of real and not real diaster Tweets.\n",
    "* **test.csv** - testing samples of real and not real diaster Tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c14dd4c",
   "metadata": {},
   "source": [
    "## Visualizing a text dataset\n",
    "\n",
    "Once you've acquired a new dataset to work with, what should you do first?\n",
    "\n",
    "Explore it? Inspect it? Verify it? Become one with it?\n",
    "\n",
    "All correct.\n",
    "\n",
    "Remember the motto: visualize, visualize, visualize.\n",
    "\n",
    "Right now, our text data samples are in the form of `.csv` files. For an easy way to make them visual, let's turn them into pandas DataFrame's.\n",
    "\n",
    "üìñ **Reading**: You might come across text datasets in many different formats. Aside from CSV files (what we're working with), you'll probably encounter `.txt` files and `.json` files too. For working with these type of files, I'd recommend reading the two following articles by RealPython:\n",
    "\n",
    "[How to Read and Write Files in Python](https://realpython.com/read-write-files-python/)\n",
    "[Working with JSON Data in Python](https://realpython.com/python-json/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84dfa0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn .csv files into pandas DataFrame's\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee67084a",
   "metadata": {},
   "source": [
    "The training data we downloaded is probably shuffled already. But just to be sure, let's shuffle it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4c53113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ¬â√õ√èThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ¬â√õ√èThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle training dataframe\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42) # shuffle with random_state=42 for reproducibility\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ede9135",
   "metadata": {},
   "source": [
    "Notice how the training data has a \"target\" column.\n",
    "\n",
    "We're going to be writing code to find patterns (e.g. different combinations of words) in the \"text\" column of the training dataset to predict the value of the \"target\" column.\n",
    "\n",
    "The test dataset doesn't have a \"target\" column.\n",
    "\n",
    "Inputs (text column) -> Machine Learning Algorithm -> Outputs (target column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3337a6",
   "metadata": {},
   "source": [
    "![rnn.png](Images/ran.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "950980d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The test data doesn't have a target (that's what we'd try to predict)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdceaac",
   "metadata": {},
   "source": [
    "Let's check how many examples of each target we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cf2ee21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many examples of each class?\n",
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc291f3",
   "metadata": {},
   "source": [
    "Since we have two target values, we're dealing with a binary classification problem.\n",
    "\n",
    "It's fairly balanced too, about `60%` negative class (target = 0) and `40%` positive class (target = 1).\n",
    "\n",
    "Where,\n",
    "\n",
    "`1` = a real disaster Tweet\n",
    "`0` = not a real disaster Tweet\n",
    "And what about the total number of samples we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04fe8eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 7613\n",
      "Total test samples: 3263\n",
      "Total samples: 10876\n"
     ]
    }
   ],
   "source": [
    "# How many samples total?\n",
    "print(f\"Total training samples: {len(train_df)}\")\n",
    "print(f\"Total test samples: {len(test_df)}\")\n",
    "print(f\"Total samples: {len(train_df) + len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549fff0c",
   "metadata": {},
   "source": [
    "Alright, seems like we've got a decent amount of training and test data. If anything, we've got an abundance of testing examples, usually a split of 90/10 (90% training, 10% testing) or 80/20 is suffice.\n",
    "\n",
    "Okay, time to visualize, let's write some code to visualize random text samples.\n",
    "\n",
    "ü§î Question: Why visualize random samples? You could visualize samples in order but this could lead to only seeing a certain subset of data. Better to visualize a substantial quantity (100+) of random samples to get an idea of the different kinds of data you're working with. In machine learning, never underestimate the power of randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bb8d27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "Not being able to touch anything or anyone in Penneys without being electrocuted ??\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Watching Xela firefighters struggle to save burning buildings last night w/ old equipment makes me so grateful for DCFD @ChR3lyc @IAFF36\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "Injured Barcelona full-back Alba out of Super Cup http://t.co/jYiEgtnC6H\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "RNK issues Severe Thunderstorm Warning [wind: 60 MPH hail: 1.00 IN] for Rockingham Stokes [NC] till Aug 5 10:00 PM EDT  ¬â√õ_\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "Womens Handbags Cross Body Geometric Pattern Satchel Totes Shoulder Bags White http://t.co/L1GFXgOZvx http://t.co/TKJYbjjsKl\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's visualize some random training examples\n",
    "import random\n",
    "random_index = random.randint(0, len(train_df)-5) # create random indexes not higher than the total number of samples\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n",
    "  _, text, target = row\n",
    "  print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
    "  print(f\"Text:\\n{text}\\n\")\n",
    "  print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79015380",
   "metadata": {},
   "source": [
    "## Split data into training and validation sets\n",
    "\n",
    "Since the test set has no labels and we need a way to evalaute our trained models, we'll split off some of the training data and create a validation set.\n",
    "\n",
    "When our model trains (tries patterns in the Tweet samples), it'll only see data from the training set and we can see how it performs on unseen data using the validation set.\n",
    "\n",
    "We'll convert our splits from pandas Series datatypes to lists of strings (for the text) and lists of ints (for the labels) for ease of use later.\n",
    "\n",
    "To split our training dataset and create a validation dataset, we'll use Scikit-Learn's `train_test_split()` method and dedicate 10% of the training samples to the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58d5263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use train_test_split to split training data into training and validation sets\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
    "                                                                            train_df_shuffled[\"target\"].to_numpy(),\n",
    "                                                                            test_size=0.1, # dedicate 10% of samples to validation set\n",
    "                                                                            random_state=42) # random state for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c9905ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 6851, 762, 762)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the lengths\n",
    "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f89af74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first 10 training sentences and their labels\n",
    "train_sentences[:10], train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea559f7",
   "metadata": {},
   "source": [
    "## Converting text into numbers\n",
    "\n",
    "Wonderful! We've got a training set and a validation set containing Tweets and labels.\n",
    "\n",
    "Our **labels are in numerical form (0 and 1) but our Tweets are in string form.**\n",
    "\n",
    "ü§î **Question**: What do you think we have to do before we can use a machine learning algorithm with our text data?\n",
    "\n",
    "If you answered something along the lines of \"turn it into numbers\", you're correct. A machine learning algorithm requires its inputs to be in numerical form.\n",
    "\n",
    "In NLP, there are two main concepts for turning text into numbers:\n",
    "\n",
    "* **Tokenization** - A straight mapping from word or character or sub-word to a numerical value. There are three main levels of tokenization:\n",
    "1. Using **word-level tokenization** with the sentence \"I love TensorFlow\" might result in \"I\" being 0, \"love\" being 1 and \"TensorFlow\" being 2. In this case, every word in a sequence considered a single token.\n",
    "2. **Character-level tokenization**, such as converting the letters A-Z to values 1-26. In this case, every character in a sequence considered a single token.\n",
    "3. **Sub-word tokenization** is in between word-level and character-level tokenization. It involves breaking invidual words into smaller parts and then converting those smaller parts into numbers. For example, \"my favourite food is pineapple pizza\" might become \"my, fav, avour, rite, fo, oo, od, is, pin, ine, app, le, piz, za\". After doing this, these sub-words would then be mapped to a numerical value. In this case, every word could be considered multiple tokens.\n",
    "* **Embeddings** - An embedding is a representation of natural language which can be learned. Representation comes in the form of a feature vector. For example, the word \"dance\" could be represented by the 5-dimensional vector [-0.8547, 0.4559, -0.3332, 0.9877, 0.1112]. It's important to note here, the size of the feature vector is tuneable. There are two ways to use embeddings:\n",
    "1. **Create your own embedding** - Once your text has been turned into numbers (required for an embedding), you can put them through an embedding layer (such as `tf.keras.layers.Embedding`) and an embedding representation will be learned during model training.\n",
    "2. **Reuse a pre-learned embedding** - Many pre-trained embeddings exist online. These pre-trained embeddings have often been learned on large corpuses of text (such as all of Wikipedia) and thus have a good underlying representation of natural language. You can use a `pre-trained embedding` to initialize your model and fine-tune it to your own specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21ab99",
   "metadata": {},
   "source": [
    "![text2numbers.png](Images/text2numbers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56690890",
   "metadata": {},
   "source": [
    "ü§î **Question**: What level of tokenzation should I use? What embedding should should I choose?\n",
    "\n",
    "It depends on your problem. You could try character-level tokenization/embeddings and word-level tokenization/embeddings and see which perform best. You might even want to try stacking them (e.g. combining the outputs of your embedding layers using `[tf.keras.layers.concatenate](https://www.tensorflow.org/api_docs/python/tf/keras/layers/concatenate)`).\n",
    "\n",
    "If you're looking for **pre-trained word embeddings, [Word2vec embeddings](http://jalammar.github.io/illustrated-word2vec/), [GloVe embeddings](https://nlp.stanford.edu/projects/glove/)** and many of the options available on [TensorFlow Hub](https://tfhub.dev/s?module-type=text-embedding) are great places to start.\n",
    "\n",
    "üîë Note: Much like searching for a pre-trained computer vision model, you can search for pre-trained word embeddings to use for your problem. Try searching for something like \"use pre-trained word embeddings in TensorFlow\".\n",
    "\n",
    "## Text vectorization (tokenization)\n",
    "\n",
    "Enough talking about tokenization and embeddings, let's create some.\n",
    "\n",
    "We'll practice tokenzation (mapping our words to numbers) first.\n",
    "\n",
    "To tokenize our words, we'll use the helpful preprocessing layer `tf.keras.layers.experimental.preprocessing.TextVectorization`.\n",
    "\n",
    "#### The TextVectorization layer takes the following parameters:\n",
    "\n",
    "* **max_tokens** - The maximum number of words in your vocabulary (e.g. 20000 or the number of unique words in your text), includes a value for OOV (out of vocabulary) tokens.\n",
    "* **standardize** - Method for standardizing text. Default is \"lower_and_strip_punctuation\" which lowers text and removes all punctuation marks.\n",
    "* **split** - How to split text, default is \"whitespace\" which splits on spaces.\n",
    "* **ngrams** - How many words to contain per token split, for example, ngrams=2 splits tokens into continuous sequences of 2.\n",
    "* **output_mode** - How to output tokens, can be \"int\" (integer mapping), \"binary\" (one-hot encoding), \"count\" or \"tf-idf\". See documentation for more.\n",
    "* **output_sequence_length** - Length of tokenized sequence to output. For example, if output_sequence_length=150, all tokenized sequences will be 150 tokens long.\n",
    "* **pad_to_max_tokens** - Defaults to False, if True, the output feature axis will be padded to max_tokens even if the number of unique tokens in the vocabulary is less than max_tokens. Only valid in certain modes, see docs for more.\n",
    "\n",
    "Let's see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bf61945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 03:09:52.502081: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-12-15 03:09:52.502344: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "# Note: in TensorFlow 2.6+, you no longer need \"layers.experimental.preprocessing\"\n",
    "# you can use: \"tf.keras.layers.TextVectorization\", see https://github.com/tensorflow/tensorflow/releases/tag/v2.6.0 for more\n",
    "\n",
    "# Use the default TextVectorization variables\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n",
    "                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
    "                                    split=\"whitespace\", # how to split tokens\n",
    "                                    ngrams=None, # create groups of n-words?\n",
    "                                    output_mode=\"int\", # how to map tokens to numbers\n",
    "                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n",
    "                                    #pad_to_max_tokens=True) # Not valid if using max_tokens=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60af9d0",
   "metadata": {},
   "source": [
    "We've initialized a `TextVectorization` object with the default settings but let's customize it a little bit for our own use case.\n",
    "\n",
    "In particular, let's set values for `max_tokens` and `output_sequence_length`.\n",
    "\n",
    "For `max_tokens` (the number of words in the vocabulary), multiples of 10,000 (10,000, 20,000, 30,000) or the exact number of unique words in your text (e.g. 32,179) are common values.\n",
    "\n",
    "For our use case, we'll use 10,000.\n",
    "\n",
    "And for the `output_sequence_length` we'll use the average number of tokens per Tweet in the training set. But first, we'll need to find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b67d3187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find average number of tokens (words) in training Tweets\n",
    "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cac138",
   "metadata": {},
   "source": [
    "Now let's create another TextVectorization object using our custom parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "095b32c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup text vectorization with custom variables\n",
    "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
    "max_length = 15 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                    output_mode=\"int\",\n",
    "                                    output_sequence_length=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bbf3d6",
   "metadata": {},
   "source": [
    "Beautiful!\n",
    "\n",
    "To map our `TextVectorization` instance `text_vectorizer` to our data, we can call the `adapt()` method on it whilst passing it our training text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f82abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the text vectorizer to the training text\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a7a20",
   "metadata": {},
   "source": [
    "Training data mapped! Let's try our `text_vectorizer` on a custom sentence (one similar to what you might see in the training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0fe213e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]])>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sample sentence and tokenize it\n",
    "sample_sentence = \"There's a flood in my street!\"\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589555d9",
   "metadata": {},
   "source": [
    "Wonderful, it seems we've got a way to turn our text into numbers (in this case, word-level tokenization). Notice the 0's at the end of the returned tensor, this is because we set `output_sequence_length`=15, meaning no matter the size of the sequence we pass to `text_vectorizer`, it always returns a sequence with a length of 15.\n",
    "\n",
    "How about we try our `text_vectorizer` on a few random sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab9b87ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\n",
      " \n",
      "#FARRAKHAN #QUOTE      \n",
      "\n",
      "Vectorized version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[  65,    9, 1710,    2, 2255,    7,  207,   65,    9, 1710,    2,\n",
       "        1149,  334,  200,    9]])>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a random sentence from the training dataset and tokenize it\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nVectorized version:\")\n",
    "text_vectorizer([random_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd787f",
   "metadata": {},
   "source": [
    "Looking good!\n",
    "\n",
    "Finally, we can check the unique tokens in our vocabulary using the `get_vocabulary()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "520f7106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 10000\n",
      "Top 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
      "Bottom 5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
     ]
    }
   ],
   "source": [
    "# Get the unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)\n",
    "bottom_5_words = words_in_vocab[-5:] # least common tokens\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"Top 5 most common words: {top_5_words}\") \n",
    "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52de2cf",
   "metadata": {},
   "source": [
    "## Creating an Embedding using an Embedding Layer\n",
    "\n",
    "We've got a way to map our text to numbers. How about we go a step further and turn those numbers into an embedding?\n",
    "\n",
    "The powerful thing about an embedding is it can be learned during training. This means rather than just being static (e.g. 1 = I, 2 = love, 3 = TensorFlow), a word's numeric representation can be improved as a model goes through data samples.\n",
    "\n",
    "We can see what an embedding of a word looks like by using the `tf.keras.layers.Embedding` layer.\n",
    "\n",
    "The main parameters we're concerned about here are:\n",
    "\n",
    "* `input_dim` - The size of the vocabulary (e.g. `len(text_vectorizer.get_vocabulary()`).\n",
    "* `output_dim` - The size of the output embedding vector, for example, a value of 100 outputs a feature vector of size 100 for each word.\n",
    "* `embeddings_initializer` - How to initialize the embeddings matrix, default is `\"uniform\"` which randomly initalizes embedding matrix with uniform distribution. This can be changed for using pre-learned embeddings.\n",
    "* `input_length` - Length of sequences being passed to embedding layer.\n",
    "\n",
    "Knowing these, let's make an embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c343182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.embeddings.Embedding at 0x17ffc2250>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
    "                             output_dim=128, # set size of embedding vector\n",
    "                             embeddings_initializer=\"uniform\", # default, intialize randomly\n",
    "                             input_length=max_length, # how long is each input\n",
    "                             name=\"embedding_1\") \n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b378941",
   "metadata": {},
   "source": [
    "Excellent, notice how `embedding` is a TensoFlow layer? This is important because we can use it as part of a model, meaning its parameters (word representations) can be updated and improved as the model learns.\n",
    "\n",
    "How about we try it out on a sample sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a018b3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "@JackMulholland1 I think also became THE MARQUIS! Then Carlos &amp; Charlie's and finally Dublin's. Sadly demolished.      \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[ 0.03977952, -0.03782602, -0.03646283, ...,  0.00236253,\n",
       "          0.03332629,  0.02803668],\n",
       "        [ 0.03494309,  0.02677599, -0.01469069, ..., -0.00333368,\n",
       "         -0.02596296, -0.02523291],\n",
       "        [-0.03618741, -0.03685092, -0.02795172, ..., -0.03069264,\n",
       "         -0.02169815, -0.04408542],\n",
       "        ...,\n",
       "        [ 0.03086955,  0.00844703,  0.00826192, ...,  0.00080078,\n",
       "          0.04420408, -0.00509788],\n",
       "        [ 0.03977952, -0.03782602, -0.03646283, ...,  0.00236253,\n",
       "          0.03332629,  0.02803668],\n",
       "        [-0.02669332,  0.02052274, -0.04391098, ..., -0.00742565,\n",
       "         -0.03833497, -0.00493672]]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a random sentence from training set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nEmbedded version:\")\n",
    "\n",
    "# Embed the random sentence (turn it into numerical representation)\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de85dc",
   "metadata": {},
   "source": [
    "Each token in the sentence gets turned into a length 128 feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b396114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       "array([ 0.03977952, -0.03782602, -0.03646283, -0.02449075, -0.00015752,\n",
       "        0.02220254,  0.00162981,  0.00603487,  0.0085157 , -0.02620113,\n",
       "        0.04101599,  0.03715892,  0.02397566,  0.00281113, -0.02704906,\n",
       "       -0.04870148,  0.01457943,  0.0059551 , -0.02334484,  0.03581132,\n",
       "        0.04377897,  0.04186075,  0.03245703, -0.045092  ,  0.04260418,\n",
       "        0.03398135, -0.01812425, -0.03539513,  0.02954218,  0.02556742,\n",
       "       -0.03345481,  0.04272738, -0.00798845, -0.0406163 , -0.00644834,\n",
       "        0.00232404,  0.01703629,  0.03645121, -0.02622857,  0.03498118,\n",
       "       -0.03059715,  0.02576998, -0.04221511,  0.02654583, -0.02192564,\n",
       "       -0.0346157 ,  0.00075326,  0.01427345,  0.01027539, -0.04311384,\n",
       "       -0.03973336, -0.00966626,  0.01032177, -0.04011822, -0.018892  ,\n",
       "       -0.01233201,  0.02721632, -0.01232889, -0.02504088, -0.04715574,\n",
       "        0.00558523, -0.00801403,  0.03058865, -0.01923352, -0.04175536,\n",
       "       -0.03654208, -0.04277095, -0.0033918 , -0.04226271,  0.0240727 ,\n",
       "        0.01274442, -0.00268712,  0.03158386,  0.04823284,  0.02940297,\n",
       "        0.00636031,  0.01719667,  0.03907609, -0.0249153 , -0.00834242,\n",
       "        0.03881217, -0.04461795,  0.03740455, -0.02412283, -0.01538421,\n",
       "        0.03984089, -0.04073881,  0.0317078 , -0.04241145, -0.03435747,\n",
       "        0.03021734,  0.0443267 ,  0.00424304, -0.01204874, -0.00213076,\n",
       "       -0.02668054,  0.0461436 , -0.04644201,  0.02519932, -0.04503984,\n",
       "       -0.03163396, -0.02846084,  0.01276297,  0.02938429,  0.03437588,\n",
       "        0.00973482, -0.01860742,  0.03510927, -0.04996962,  0.01983938,\n",
       "        0.00642185,  0.00372197,  0.01576127,  0.01591486, -0.04195998,\n",
       "        0.02777341,  0.00619875, -0.02269079, -0.00224097,  0.0289592 ,\n",
       "        0.04377018, -0.03377642,  0.00907839, -0.01134553, -0.02910458,\n",
       "        0.00236253,  0.03332629,  0.02803668], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out a single token's embedding\n",
    "sample_embed[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c4e504",
   "metadata": {},
   "source": [
    "These values might not mean much to us but they're what our computer sees each word as. When our model looks for patterns in different samples, these values will be updated as necessary.\n",
    "\n",
    "üîë Note: The previous two concepts (tokenization and embeddings) are the foundation for many NLP tasks. So if you're not sure about anything, be sure to research and conduct your own experiments to further help your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e2b8d",
   "metadata": {},
   "source": [
    "![modelingText.png](Images/modelingText.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ca7184",
   "metadata": {},
   "source": [
    "Now that we've got a way to turn our text data into numbers, we can start to build machine learning models to model it.\n",
    "\n",
    "To get plenty of practice, we're going to build a series of different models, each as its own experiment. We'll then compare the results of each model and see which one performed best.\n",
    "\n",
    "More specifically, we'll be building the following:\n",
    "\n",
    "* **Model 0**: Naive Bayes (baseline)\n",
    "* **Model 1**: Feed-forward neural network (dense model)\n",
    "* **Model 2**: LSTM model\n",
    "* **Model 3**: GRU model\n",
    "* **Model 4**: Bidirectional-LSTM model\n",
    "* **Model 5**: 1D Convolutional Neural Network\n",
    "* **Model 6**: TensorFlow Hub Pretrained Feature Extractor\n",
    "* **Model 7**: Same as model 6 with 10% of training data\n",
    "\n",
    "`Model 0` is the simplest to acquire a baseline which we'll expect each other of the other deeper models to beat.\n",
    "\n",
    "Each experiment will go through the following steps:\n",
    "\n",
    "1. Construct the model\n",
    "2. Train the model\n",
    "3. Make predictions with the model\n",
    "4. Track prediction evaluation metrics for later comparison\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b381797f",
   "metadata": {},
   "source": [
    "## Model 0: Getting a baseline\n",
    "### Naive Bayes with TF-IDF encoder (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "As with all machine learning modelling experiments, it's important to create a baseline model so you've got a benchmark for future experiments to build upon.\n",
    "\n",
    "To create our baseline, we'll create a Scikit-Learn Pipeline using the TF-IDF (term frequency-inverse document frequency) formula to convert our words to numbers and then model them with the [Multinomial Naive Bayes algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB). This was chosen via referring to the [Scikit-Learn machine learning map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html).\n",
    "\n",
    "üìñ **Reading**: The ins and outs of TF-IDF algorithm is beyond the scope of this notebook, however, the curious reader is encouraged to check out the [Scikit-Learn documentation](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting) for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7582fc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenization and modelling pipeline\n",
    "model_0 = Pipeline([\n",
    "                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
    "                    (\"clf\", MultinomialNB()) # model the text\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7785918",
   "metadata": {},
   "source": [
    "The benefit of using a shallow model like Multinomial Naive Bayes is that training is very fast.\n",
    "\n",
    "Let's evaluate our model and find our baseline metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "781d9285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our baseline model achieves an accuracy of: 79.27%\n"
     ]
    }
   ],
   "source": [
    "baseline_score = model_0.score(val_sentences, val_labels)\n",
    "print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa778f8e",
   "metadata": {},
   "source": [
    "How about we make some predictions with our baseline model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69f17048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772545f6",
   "metadata": {},
   "source": [
    "## Creating an evaluation function for our model experiments\n",
    "\n",
    "We could evaluate these as they are but since we're going to be evaluating several models in the same way going forward, let's create a helper function which takes an array of predictions and ground truth labels and computes the following:\n",
    "\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F1-score\n",
    "\n",
    "üîë Note: Since we're dealing with a classification problem, the above metrics are the most appropriate. If we were working with a regression problem, other metrics such as MAE (mean absolute error) would be a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4bd7c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate: accuracy, precision, recall, f1-score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def calculate_results(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
    "\n",
    "  Args:\n",
    "  -----\n",
    "  y_true = true labels in the form of a 1D array\n",
    "  y_pred = predicted labels in the form of a 1D array\n",
    "\n",
    "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
    "  \"\"\"\n",
    "  # Calculate model accuracy\n",
    "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "  # Calculate model precision, recall and f1 score using \"weighted\" average\n",
    "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "  model_results = {\"accuracy\": model_accuracy,\n",
    "                  \"precision\": model_precision,\n",
    "                  \"recall\": model_recall,\n",
    "                  \"f1\": model_f1}\n",
    "  return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f978510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get baseline results\n",
    "baseline_results = calculate_results(y_true=val_labels,\n",
    "                                     y_pred=baseline_preds)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2fe902",
   "metadata": {},
   "source": [
    "## Model 1: A simple dense model\n",
    "\n",
    "The first \"deep\" model we're going to build is a single layer dense model. In fact, it's barely going to have a single layer.\n",
    "\n",
    "It'll take our text and labels as input, tokenize the text, create an embedding, find the average of the embedding (using Global Average Pooling) and then pass the average through a fully connected layer with one output unit and a sigmoid activation function.\n",
    "\n",
    "If the previous sentence sounds like a mouthful, it'll make sense when we code it out (remember, if in doubt, code it out).\n",
    "\n",
    "And since we're going to be building a number of TensorFlow deep learning models, we'll import our `create_tensorboard_callback()` function from `helper_functions.py` to keep track of the results of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6cdd8745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensorboard callback (need to create a new one for each model)\n",
    "from helper_functions import create_tensorboard_callback\n",
    "\n",
    "# Create directory to save TensorBoard logs\n",
    "SAVE_DIR = \"model_logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611632ae",
   "metadata": {},
   "source": [
    "Now we've got a TensorBoard callback function ready to go, let's build our first deep model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f149432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with the Functional API\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\") # inputs are 1-dimensional strings\n",
    "x = text_vectorizer(inputs) # turn the input text into numbers\n",
    "x = embedding(x) # create an embedding of the numerized numbers\n",
    "x = layers.GlobalAveragePooling1D()(x) # lower the dimensionality of the embedding (try running the model without this layer and see what happens)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # create the output layer, want binary outputs so use sigmoid activation\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\") # construct the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a577e8",
   "metadata": {},
   "source": [
    "Looking good. Our model takes a 1-dimensional string as input (in our case, a Tweet), it then tokenizes the string using `text_vectorizer` and creates an embedding using `embedding`.\n",
    "\n",
    "We then (optionally) pool the outputs of the embedding layer to reduce the dimensionality of the tensor we pass to the output layer.\n",
    "\n",
    "üõ† **Exercise**: Try building `model_1` with and without a `GlobalAveragePooling1D()` layer after the embedding layer. What happens? Why do you think this is?\n",
    "\n",
    "Finally, we pass the output of the pooling layer to a `dense` layer with `sigmoid` activation (we use sigmoid since our problem is binary classification).\n",
    "\n",
    "Before we can fit our model to the data, we've got to compile it. Since we're working with binary classification, we'll use \"`binary_crossentropy`\" as our loss function and the `Adam` optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3984a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_1.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4895689a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization_1 (TextVe (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 15, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get a summary of the model\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d6538b",
   "metadata": {},
   "source": [
    "Most of the trainable parameters are contained within the embedding layer. Recall we created an embedding of size 128 (output_dim=128) for a vocabulary of size 10,000 (input_dim=10000), hence the 1,280,000 trainable parameters.\n",
    "\n",
    "Alright, our model is compiled, let's fit it to our training data for 5 epochs. We'll also pass our TensorBoard callback function to make sure our model's training metrics are logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da527377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 10:07:17.221375: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
      "2022-12-15 10:07:17.221692: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n",
      "2022-12-15 10:07:17.223272: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/simple_dense_model/20221215-100717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 10:07:17.502531: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-12-15 10:07:17.513881: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 10:07:17.948967: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/215 [..............................] - ETA: 10s - loss: 0.6924 - accuracy: 0.5063 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 10:07:22.898097: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
      "2022-12-15 10:07:22.898115: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n",
      "2022-12-15 10:07:22.927291: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-12-15 10:07:22.931600: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n",
      "2022-12-15 10:07:22.939495: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: model_logs/simple_dense_model/20221215-100717/train/plugins/profile/2022_12_15_10_07_22\n",
      "2022-12-15 10:07:22.941311: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to model_logs/simple_dense_model/20221215-100717/train/plugins/profile/2022_12_15_10_07_22/SNIGDHAs-MacBook-Air-2.local.trace.json.gz\n",
      "2022-12-15 10:07:22.954357: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: model_logs/simple_dense_model/20221215-100717/train/plugins/profile/2022_12_15_10_07_22\n",
      "2022-12-15 10:07:22.954710: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to model_logs/simple_dense_model/20221215-100717/train/plugins/profile/2022_12_15_10_07_22/SNIGDHAs-MacBook-Air-2.local.memory_profile.json.gz\n",
      "2022-12-15 10:07:22.955365: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: model_logs/simple_dense_model/20221215-100717/train/plugins/profile/2022_12_15_10_07_22Dumped tool data for xplane.pb to model_logs/simple_dense_model/20221215-100717/train/plugins/profile/2022_12_15_10_07_22/SNIGDHAs-MacBook-Air-2.local.xplane.pb\n",
      "Dumped tool data for overview_page.pb to model_logs/simple_dense_model/20221215-100717/train/plugins/profile/2022_12_15_10_07_22/SNIGDHAs-MacBook-Air-2.local.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to model_logs/simple_dense_model/20221215-100717/train/plugins/profile/2022_12_15_10_07_22/SNIGDHAs-MacBook-Air-2.local.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to model_logs/simple_dense_model/20221215-100717/train/plugins/profile/2022_12_15_10_07_22/SNIGDHAs-MacBook-Air-2.local.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to model_logs/simple_dense_model/20221215-100717/train/plugins/profile/2022_12_15_10_07_22/SNIGDHAs-MacBook-Air-2.local.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - ETA: 0s - loss: 0.6101 - accuracy: 0.6892"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 10:07:31.883288: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - 15s 44ms/step - loss: 0.6101 - accuracy: 0.6892 - val_loss: 0.5361 - val_accuracy: 0.7493\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 9s 40ms/step - loss: 0.4413 - accuracy: 0.8196 - val_loss: 0.4694 - val_accuracy: 0.7835\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 8s 39ms/step - loss: 0.3465 - accuracy: 0.8591 - val_loss: 0.4593 - val_accuracy: 0.7887\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 8s 36ms/step - loss: 0.2850 - accuracy: 0.8929 - val_loss: 0.4643 - val_accuracy: 0.7900\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 9s 40ms/step - loss: 0.2381 - accuracy: 0.9127 - val_loss: 0.4768 - val_accuracy: 0.7874\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_1_history = model_1.fit(train_sentences, # input sentences can be a list of strings due to text preprocessing layer built-in model\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels),\n",
    "                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR, \n",
    "                                                                     experiment_name=\"simple_dense_model\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d095e75",
   "metadata": {},
   "source": [
    "Nice! Since we're using such a simple model, each epoch processes very quickly.\n",
    "\n",
    "Let's check our model's performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb2bed6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 19ms/step - loss: 0.4768 - accuracy: 0.7874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4768421947956085, 0.787401556968689]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the results\n",
    "model_1.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11cf5737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_1/embeddings:0' shape=(10000, 128) dtype=float32, numpy=\n",
       " array([[-0.00158172, -0.0273298 ,  0.00756159, ..., -0.04513898,\n",
       "         -0.01140978,  0.02000963],\n",
       "        [ 0.04377558, -0.03518281, -0.03975828, ...,  0.0025443 ,\n",
       "          0.03344619,  0.02856332],\n",
       "        [ 0.01005246,  0.00876532,  0.03409398, ..., -0.060405  ,\n",
       "         -0.04445601,  0.01031738],\n",
       "        ...,\n",
       "        [-0.03301444, -0.0052493 , -0.04209725, ...,  0.02028764,\n",
       "          0.00308807,  0.02215792],\n",
       "        [ 0.00430909, -0.02876636,  0.06074647, ..., -0.05834114,\n",
       "         -0.01486104,  0.03736725],\n",
       "        [-0.03821995, -0.07506637,  0.05878074, ..., -0.0218875 ,\n",
       "         -0.08919894,  0.01873792]], dtype=float32)>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51b2ab56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\n",
    "print(embed_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542383ba",
   "metadata": {},
   "source": [
    "And since we tracked our model's training logs with TensorBoard, how about we visualize them?\n",
    "\n",
    "We can do so by uploading our TensorBoard log files (contained in the `model_logs` directory) to [TensorBoard.dev](https://tensorboard.dev/).\n",
    "\n",
    "üîë Note: Remember, whatever you upload to TensorBoard.dev becomes public. If there are training logs you don't want to share, don't upload them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54b0cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # View tensorboard logs of transfer learning modelling experiments (should be 4 models)\n",
    "# # Upload TensorBoard dev records\n",
    "# !tensorboard dev upload --logdir ./model_logs \\\n",
    "#   --name \"First deep model on text data\" \\\n",
    "#   --description \"Trying a dense model with an embedding layer\" \\\n",
    "#   --one_shot # exits the uploader when upload has finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "032ce929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to remove previous experiments, you can do so using the following command\n",
    "# !tensorboard dev delete --experiment_id EXPERIMENT_ID_TO_DELETE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5426e313",
   "metadata": {},
   "source": [
    "The TensorBoard.dev experiment for our first deep model can be viewed here: https://tensorboard.dev/experiment/5d1Xm10aT6m6MgyW3HAGfw/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb23dd5",
   "metadata": {},
   "source": [
    "What the training curves of our model look like on TensorBoard. From looking at the curves can you tell if the model is overfitting or underfitting?\n",
    "\n",
    "Beautiful! Those are some colorful training curves. Would you say the model is overfitting or underfitting?\n",
    "\n",
    "We've built and trained our first deep model, the next step is to make some predictions with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e69750d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 10:10:59.468055: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.41251785],\n",
       "       [0.7430053 ],\n",
       "       [0.9977781 ],\n",
       "       [0.11020958],\n",
       "       [0.1109415 ],\n",
       "       [0.93305194],\n",
       "       [0.9124672 ],\n",
       "       [0.9927475 ],\n",
       "       [0.9693494 ],\n",
       "       [0.26432115]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions (these come back in the form of probabilities)\n",
    "model_1_pred_probs = model_1.predict(val_sentences)\n",
    "model_1_pred_probs[:10] # only print out the first 10 prediction probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333a7c3d",
   "metadata": {},
   "source": [
    "Since our final layer uses a sigmoid activation function, we get our predictions back in the form of probabilities.\n",
    "\n",
    "To convert them to prediction classes, we'll use tf.round(), meaning prediction probabilities below 0.5 will be rounded to 0 and those above 0.5 will be rounded to 1.\n",
    "\n",
    "üîë Note: In practice, the output threshold of a sigmoid prediction probability doesn't necessarily have to 0.5. For example, through testing, you may find that a cut off of 0.25 is better for your chosen evaluation metrics. A common example of this threshold cutoff is the [precision-recall tradeoff](https://en.wikipedia.org/wiki/Precision_and_recall#Introduction) (search for the keyword \"tradeoff\" to learn about the phenomenon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a6d9314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn prediction probabilities into single-dimension tensor of floats\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs)) # squeeze removes single dimensions\n",
    "model_1_preds[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50484b85",
   "metadata": {},
   "source": [
    "Now we've got our model's predictions in the form of classes, we can use our `calculate_results()` function to compare them to the ground truth validation labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d14d6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.74015748031496,\n",
       " 'precision': 0.7914920592553047,\n",
       " 'recall': 0.7874015748031497,\n",
       " 'f1': 0.7846966492209201}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model_1 metrics\n",
    "model_1_results = calculate_results(y_true=val_labels, \n",
    "                                    y_pred=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eced93",
   "metadata": {},
   "source": [
    "How about we compare our first deep model to our baseline model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62534226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is our simple Keras model better than our baseline model?\n",
    "import numpy as np\n",
    "np.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b626b6",
   "metadata": {},
   "source": [
    "Since we'll be doing this kind of comparison (baseline compared to new model) quite a few times, let's create a function to help us out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f58f5ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 79.27, New accuracy: 78.74, Difference: -0.52\n",
      "Baseline precision: 0.81, New precision: 0.79, Difference: -0.02\n",
      "Baseline recall: 0.79, New recall: 0.79, Difference: -0.01\n",
      "Baseline f1: 0.79, New f1: 0.78, Difference: -0.00\n"
     ]
    }
   ],
   "source": [
    "# Create a helper function to compare our baseline results to new model results\n",
    "def compare_baseline_to_new_results(baseline_results, new_model_results):\n",
    "  for key, value in baseline_results.items():\n",
    "    print(f\"Baseline {key}: {value:.2f}, New {key}: {new_model_results[key]:.2f}, Difference: {new_model_results[key]-value:.2f}\")\n",
    "\n",
    "compare_baseline_to_new_results(baseline_results=baseline_results, \n",
    "                                new_model_results=model_1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c1584",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
